{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.models.explainer.attention_explainer failed: Traceback (most recent call last):\n",
      "  File \"/Users/ryanhung/anaconda3/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 273, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/ryanhung/anaconda3/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 471, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/Users/ryanhung/anaconda3/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1004, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/ryanhung/code/paying-attention/src/models/explainer/attention_explainer.py\", line 32\n",
      "    def supports(self) -> bool:\n",
      "    ^^^\n",
      "IndentationError: expected an indented block after function definition on line 30\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from src.models.gps import GPS\n",
    "from src.models.utils.hooks import GPSHook\n",
    "from src.models.explainer.explainer_pipeline import ExplainerPipeline\n",
    "from src.data import loader\n",
    "from src.models.model import train, test\n",
    "\n",
    "from src.models.gps import GPS\n",
    "from src.models.gcn import GCN\n",
    "from src.models.explainer.explainer_pipeline import ExplainerPipeline\n",
    "from src.models.explainer.gnn_explainer import GNNExplainer\n",
    "from src.models.explainer.attention_explainer import AttentionExplainer\n",
    "from src.data import loader\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.explain import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_classes, data_networkx = loader.load_clean_bashapes(num_nodes=25, num_edges=5, num_motifs=10, laplacian_eigenvector_dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.3333333333333333\n",
      "Test accuracy: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gps_params = {\n",
    "    'pe_channels': 2,\n",
    "    'num_layers': 4,\n",
    "    'hidden_channels': 4,\n",
    "    'num_attention_heads': 1,\n",
    "    'observe_attention': True\n",
    "}\n",
    "\n",
    "gcn_params = {\n",
    "    \"hidden_channels\": 20,\n",
    "    \"num_layers\": 3\n",
    "}\n",
    "\n",
    "explainer_params = {\n",
    "    'explanation_type': 'model',\n",
    "    'node_mask_type': 'attributes',\n",
    "    'edge_mask_type': 'object',\n",
    "    'model_config': ModelConfig(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='raw',\n",
    "    )\n",
    "}\n",
    "\n",
    "explainer_pipeline = ExplainerPipeline(data, num_classes, GPS, explainer=AttentionExplainer, Hook=GPSHook, model_params=gps_params, explainer_params=explainer_params, epochs=2)\n",
    "explainer_pipeline.get_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 14.66it/s]\n"
     ]
    }
   ],
   "source": [
    "gps = GPS(data, num_classes, **gps_params)\n",
    "hook = GPSHook(gps)\n",
    "train(gps, data, epochs=2)\n",
    "hook.remove_hooks()\n",
    "hook.attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([[[0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129],\n",
       "          [0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129],\n",
       "          [0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129],\n",
       "          ...,\n",
       "          [0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129],\n",
       "          [0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129],\n",
       "          [0.0133, 0.0133, 0.0133,  ..., 0.0129, 0.0129, 0.0129]]]),\n",
       " 1: tensor([[[0.0129, 0.0084, 0.0128,  ..., 0.0108, 0.0105, 0.0111],\n",
       "          [0.0142, 0.0117, 0.0141,  ..., 0.0104, 0.0107, 0.0096],\n",
       "          [0.0130, 0.0080, 0.0129,  ..., 0.0112, 0.0109, 0.0115],\n",
       "          ...,\n",
       "          [0.0126, 0.0074, 0.0131,  ..., 0.0178, 0.0176, 0.0186],\n",
       "          [0.0131, 0.0069, 0.0135,  ..., 0.0174, 0.0170, 0.0181],\n",
       "          [0.0116, 0.0080, 0.0121,  ..., 0.0184, 0.0183, 0.0191]]]),\n",
       " 2: tensor([[[0.0135, 0.0142, 0.0136,  ..., 0.0135, 0.0135, 0.0136],\n",
       "          [0.0105, 0.0132, 0.0111,  ..., 0.0095, 0.0088, 0.0105],\n",
       "          [0.0130, 0.0104, 0.0132,  ..., 0.0133, 0.0129, 0.0138],\n",
       "          ...,\n",
       "          [0.0115, 0.0037, 0.0120,  ..., 0.0117, 0.0106, 0.0133],\n",
       "          [0.0120, 0.0046, 0.0125,  ..., 0.0121, 0.0111, 0.0136],\n",
       "          [0.0107, 0.0027, 0.0111,  ..., 0.0110, 0.0099, 0.0127]]]),\n",
       " 3: tensor([[[0.0136, 0.0196, 0.0130,  ..., 0.0104, 0.0113, 0.0094],\n",
       "          [0.0093, 0.0001, 0.0125,  ..., 0.0100, 0.0110, 0.0085],\n",
       "          [0.0125, 0.0232, 0.0118,  ..., 0.0093, 0.0099, 0.0087],\n",
       "          ...,\n",
       "          [0.0103, 0.0681, 0.0091,  ..., 0.0056, 0.0064, 0.0048],\n",
       "          [0.0111, 0.0507, 0.0100,  ..., 0.0059, 0.0068, 0.0048],\n",
       "          [0.0090, 0.0992, 0.0079,  ..., 0.0050, 0.0056, 0.0044]]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,\n",
       "          4,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11,\n",
       "         11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
       "         13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "         16, 17, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         20, 20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 23, 23, 23, 23,\n",
       "         23, 24, 24, 24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 28, 28, 29, 29,\n",
       "         14, 26, 30, 30, 30, 31, 31, 31, 32, 32, 33, 33, 34, 34, 19, 32, 35, 35,\n",
       "         35, 36, 36, 36, 37, 37, 38, 38, 39, 39, 13, 35, 40, 40, 40, 41, 41, 41,\n",
       "         42, 42, 43, 43, 44, 44,  8, 42, 45, 45, 45, 46, 46, 46, 47, 47, 48, 48,\n",
       "         49, 49,  7, 46, 50, 50, 50, 51, 51, 51, 52, 52, 53, 53, 54, 54, 16, 53,\n",
       "         55, 55, 55, 56, 56, 56, 57, 57, 58, 58, 59, 59,  4, 59, 60, 60, 60, 61,\n",
       "         61, 61, 62, 62, 63, 63, 64, 64, 23, 64, 65, 65, 65, 66, 66, 66, 67, 67,\n",
       "         68, 68, 69, 69, 21, 66, 70, 70, 70, 71, 71, 71, 72, 72, 73, 73, 74, 74,\n",
       "         22, 71],\n",
       "        [ 1,  3,  5,  6,  7, 20, 24,  0,  2,  5,  6,  7,  8,  9, 10, 12, 14, 15,\n",
       "         18, 19, 20, 21, 23,  1,  3,  5,  6, 14, 15, 17,  0,  2,  5, 11, 14, 24,\n",
       "          6,  0,  1,  2,  3,  7, 11,  0,  1,  2,  4,  7,  8,  9, 10, 11, 13, 14,\n",
       "          0,  1,  5,  6,  8, 10, 12, 16,  1,  6,  7,  9, 10, 11, 16, 21,  1,  6,\n",
       "          8, 12, 13, 17, 18, 19, 21, 23, 24,  1,  6,  7,  8, 13, 16, 18, 19,  3,\n",
       "          5,  6,  8, 16, 18, 21, 24,  1,  7,  9, 13, 14, 19, 22,  6,  9, 10, 12,\n",
       "         17,  1,  2,  3,  6, 12, 15,  1,  2, 14, 17, 19, 23,  7,  8, 10, 11, 20,\n",
       "         23,  2,  9, 13, 15, 22,  1,  9, 10, 11,  1,  9, 10, 12, 15, 20, 22, 24,\n",
       "          0,  1, 16, 19,  1,  8,  9, 11, 22, 12, 17, 19, 21, 23,  1,  9, 15, 16,\n",
       "         22,  0,  3,  9, 11, 19, 26, 28, 29, 29, 27, 25, 26, 28, 27, 25, 25, 26,\n",
       "         26, 14, 31, 33, 34, 34, 32, 30, 31, 33, 32, 30, 30, 31, 32, 19, 36, 38,\n",
       "         39, 39, 37, 35, 36, 38, 37, 35, 35, 36, 35, 13, 41, 43, 44, 44, 42, 40,\n",
       "         41, 43, 42, 40, 40, 41, 42,  8, 46, 48, 49, 49, 47, 45, 46, 48, 47, 45,\n",
       "         45, 46, 46,  7, 51, 53, 54, 54, 52, 50, 51, 53, 52, 50, 50, 51, 53, 16,\n",
       "         56, 58, 59, 59, 57, 55, 56, 58, 57, 55, 55, 56, 59,  4, 61, 63, 64, 64,\n",
       "         62, 60, 61, 63, 62, 60, 60, 61, 64, 23, 66, 68, 69, 69, 67, 65, 66, 68,\n",
       "         67, 65, 65, 66, 66, 21, 71, 73, 74, 74, 72, 70, 71, 73, 72, 70, 70, 71,\n",
       "         71, 22]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0135, 0.0145, 0.0134,  ..., 0.0132, 0.0133, 0.0129],\n",
      "        [0.0073, 0.0208, 0.0070,  ..., 0.0075, 0.0070, 0.0085],\n",
      "        [0.0135, 0.0143, 0.0135,  ..., 0.0135, 0.0137, 0.0132],\n",
      "        ...,\n",
      "        [0.0138, 0.0176, 0.0138,  ..., 0.0159, 0.0168, 0.0149],\n",
      "        [0.0138, 0.0176, 0.0138,  ..., 0.0154, 0.0163, 0.0145],\n",
      "        [0.0136, 0.0183, 0.0137,  ..., 0.0165, 0.0175, 0.0156]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer_pipeline\u001b[38;5;241m.\u001b[39mexplain(\u001b[38;5;241m26\u001b[39m, laplacian_eigenvector_pe \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mlaplacian_eigenvector_pe)\n\u001b[1;32m      2\u001b[0m explainer_pipeline\u001b[38;5;241m.\u001b[39mexplanations[\u001b[38;5;241m26\u001b[39m]\n",
      "File \u001b[0;32m~/code/paying-attention/src/models/explainer/explainer_pipeline.py:42\u001b[0m, in \u001b[0;36mExplainerPipeline.explain\u001b[0;34m(self, node_idx, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanations[node_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39medge_index, index\u001b[38;5;241m=\u001b[39mnode_idx, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/explain/explainer.py:250\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m             explanation[key] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 250\u001b[0m explanation\u001b[38;5;241m.\u001b[39mvalidate_masks()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39mthreshold(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/explain/explanation.py:55\u001b[0m, in \u001b[0;36mExplanationMixin.validate_masks\u001b[0;34m(self, raise_on_error)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m store:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store\u001b[38;5;241m.\u001b[39medge_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     warn_or_raise(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with one dimension (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;241m.\u001b[39medge_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, raise_on_error)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "explainer_pipeline.explain(26, laplacian_eigenvector_pe = data.laplacian_eigenvector_pe)\n",
    "explainer_pipeline.explanations[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_reduce(matrix):\n",
    "\n",
    "    #apply max reduction along columns (dim = 1)\n",
    "    max_values, _ = matrix.max(dim=1)\n",
    "    return max_values\n",
    "\n",
    "#finds the weighted_average for one singular attention matrix\n",
    "\n",
    "#when summing across dim=0, output shows how much attention each node receives, when dim=1, it shows how much attention each node is giving\n",
    "\n",
    "def weighted_average_received(attention_matrix):\n",
    "    \n",
    "    # Apply softmax across rows (dim=1) to normalize each row of the attention matrix\n",
    "    softmax_attention = F.softmax(attention_matrix, dim=1)\n",
    "    \n",
    "    # Compute the weighted average across each row (dim=1) by summing\n",
    "    weighted_avg = softmax_attention.sum(dim=0)  # Sum along the columns\n",
    "    \n",
    "    return weighted_avg\n",
    "\n",
    "def weighted_average_given(attention_matrix):\n",
    "    \"\"\"Computes the weighted average of an attention matrix to show how much attention each node is giving to others\"\"\"\n",
    "    \n",
    "    # Step 1: Apply softmax across rows (dim=1) to normalize attention\n",
    "    softmax_attention = F.softmax(attention_matrix, dim=1)\n",
    "    \n",
    "    # Step 2: Compute the weighted average across rows (dim=1)\n",
    "    # Multiply each value by its respective column index (weighted sum)\n",
    "    weighted_avg = torch.matmul(softmax_attention, torch.arange(attention_matrix.size(1), dtype=torch.float32))\n",
    "\n",
    "    return weighted_avg\n",
    "\n",
    "def weighted_average_all_layers(function, matrices):\n",
    "\n",
    "    #store all the weighted averages per matrix (from each layer)\n",
    "    weighted_averages = []\n",
    "\n",
    "    #use the weighted_average function (single matrix use case) in a loop to collect all the weighted averages,\n",
    "    #and append to list\n",
    "\n",
    "    for matrix in matrices:\n",
    "        current_weighted_avg = function(matrix)\n",
    "        weighted_averages.append(current_weighted_avg)\n",
    "\n",
    "    # Compute the average of the weighted averages across all layers\n",
    "    avg_all_matrices= torch.stack(weighted_averages).mean(dim=0)\n",
    "\n",
    "    return weighted_averages, avg_all_matrices\n",
    "\n",
    "def top_k_nodes(matrix, top_k=0.1):\n",
    "\n",
    "    # Rank nodes by importance (highest first)\n",
    "    sorted_indices = torch.argsort(matrix, descending=True)\n",
    "\n",
    "    # Select the top K nodes (either as a percentage or fixed number)\n",
    "    if isinstance(top_k, float):\n",
    "        top_k = int(len(sorted_indices) * top_k)  # Percentage to number of nodes\n",
    "    top_nodes = sorted_indices[:top_k]\n",
    "    return top_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
