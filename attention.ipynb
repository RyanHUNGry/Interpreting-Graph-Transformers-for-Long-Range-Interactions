{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from src.models.gps import GPS\n",
    "from src.models.utils.hooks import GPSHook\n",
    "from src.models.explainer.explainer_pipeline import ExplainerPipeline\n",
    "from src.data import loader\n",
    "from src.models.model import train, test\n",
    "\n",
    "from src.models.gps import GPS\n",
    "from src.models.gcn import GCN\n",
    "from src.models.explainer.explainer_pipeline import ExplainerPipeline\n",
    "from src.models.explainer.gnn_explainer import GNNExplainer\n",
    "from src.models.explainer.attention_explainer import AttentionExplainer\n",
    "from src.data import loader\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.explain import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, num_classes, data_networkx = loader.load_clean_bashapes(num_nodes=25, num_edges=5, num_motifs=10, laplacian_eigenvector_dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 66.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.26666666666666666\n",
      "Test accuracy: 0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gps_params = {\n",
    "    'pe_channels': 2,\n",
    "    'num_layers': 4,\n",
    "    'hidden_channels': 4,\n",
    "    'num_attention_heads': 1,\n",
    "    'observe_attention': True\n",
    "}\n",
    "\n",
    "gcn_params = {\n",
    "    \"hidden_channels\": 20,\n",
    "    \"num_layers\": 3\n",
    "}\n",
    "\n",
    "explainer_params = {\n",
    "    'explanation_type': 'model',\n",
    "    'node_mask_type': 'attributes',\n",
    "    'edge_mask_type': 'object',\n",
    "    'model_config': ModelConfig(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='raw',\n",
    "    )\n",
    "}\n",
    "\n",
    "explainer_pipeline = ExplainerPipeline(data, num_classes, GPS, explainer=AttentionExplainer, Hook=GPSHook, model_params=gps_params, explainer_params=explainer_params, epochs=2)\n",
    "explainer_pipeline.get_accuracies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,\n",
      "          1,  1,  1,  1,  1,  1,  2,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,\n",
      "          4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n",
      "          6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  9,\n",
      "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "         10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
      "         13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "         17, 17, 17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
      "         20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 24,\n",
      "         24, 24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 28, 28, 29, 29,  6, 26,\n",
      "         30, 30, 30, 31, 31, 31, 32, 32, 33, 33, 34, 34,  2, 33, 35, 35, 35, 36,\n",
      "         36, 36, 37, 37, 38, 38, 39, 39,  1, 36, 40, 40, 40, 41, 41, 41, 42, 42,\n",
      "         43, 43, 44, 44,  3, 41, 45, 45, 45, 46, 46, 46, 47, 47, 48, 48, 49, 49,\n",
      "         20, 48, 50, 50, 50, 51, 51, 51, 52, 52, 53, 53, 54, 54, 22, 50, 55, 55,\n",
      "         55, 56, 56, 56, 57, 57, 58, 58, 59, 59, 15, 56, 60, 60, 60, 61, 61, 61,\n",
      "         62, 62, 63, 63, 64, 64, 13, 64, 65, 65, 65, 66, 66, 66, 67, 67, 68, 68,\n",
      "         69, 69,  0, 66, 70, 70, 70, 71, 71, 71, 72, 72, 73, 73, 74, 74,  7, 73],\n",
      "        [ 1,  5,  7,  8,  9, 11, 12, 13, 14, 15, 16, 19, 20,  0,  5,  6,  7, 10,\n",
      "         14, 16, 20, 21, 22, 24,  4,  5,  6,  9, 18,  2,  5,  8, 10, 11, 13, 15,\n",
      "         18, 20,  0,  1,  3,  4,  6,  8,  9, 12, 14, 22, 23,  1,  3,  5,  7,  8,\n",
      "          9, 11, 13, 16,  0,  1,  6, 10, 13, 14, 18, 23, 24,  0,  4,  5,  6,  0,\n",
      "          3,  5,  6, 10, 15, 17, 18, 19, 21, 23,  1,  4,  7,  9, 12, 15, 17, 18,\n",
      "         19, 23,  0,  4,  6, 13, 19,  0,  5, 10, 16, 21, 22,  0,  4,  6,  7, 11,\n",
      "         21,  0,  1,  5,  7, 20,  0,  4,  9, 10, 17, 19, 23, 24,  0,  1,  6, 12,\n",
      "          9, 10, 15,  3,  4,  7,  9, 10, 24,  0,  9, 10, 11, 15, 24,  0,  1,  4,\n",
      "         14, 22,  1,  9, 12, 13, 22,  1,  5, 12, 20, 21,  5,  7,  9, 10, 15,  1,\n",
      "          7, 15, 18, 19, 26, 28, 29, 29, 27, 25, 26, 28, 27, 25, 25, 26, 26,  6,\n",
      "         31, 33, 34, 34, 32, 30, 31, 33, 32, 30, 30, 31, 33,  2, 36, 38, 39, 39,\n",
      "         37, 35, 36, 38, 37, 35, 35, 36, 36,  1, 41, 43, 44, 44, 42, 40, 41, 43,\n",
      "         42, 40, 40, 41, 41,  3, 46, 48, 49, 49, 47, 45, 46, 48, 47, 45, 45, 46,\n",
      "         48, 20, 51, 53, 54, 54, 52, 50, 51, 53, 52, 50, 50, 51, 50, 22, 56, 58,\n",
      "         59, 59, 57, 55, 56, 58, 57, 55, 55, 56, 56, 15, 61, 63, 64, 64, 62, 60,\n",
      "         61, 63, 62, 60, 60, 61, 64, 13, 66, 68, 69, 69, 67, 65, 66, 68, 67, 65,\n",
      "         65, 66, 66,  0, 71, 73, 74, 74, 72, 70, 71, 73, 72, 70, 70, 71, 73,  7]])\n",
      "tensor([[0.0158, 0.0120, 0.0147,  ..., 0.0140, 0.0137, 0.0142],\n",
      "        [0.0125, 0.0113, 0.0159,  ..., 0.0136, 0.0133, 0.0136],\n",
      "        [0.0199, 0.0169, 0.0090,  ..., 0.0123, 0.0126, 0.0124],\n",
      "        ...,\n",
      "        [0.0138, 0.0134, 0.0126,  ..., 0.0134, 0.0134, 0.0138],\n",
      "        [0.0133, 0.0130, 0.0130,  ..., 0.0134, 0.0133, 0.0137],\n",
      "        [0.0135, 0.0131, 0.0126,  ..., 0.0132, 0.0132, 0.0137]])\n",
      "torch.Size([75, 75])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer_pipeline\u001b[38;5;241m.\u001b[39mexplain(\u001b[38;5;241m26\u001b[39m, laplacian_eigenvector_pe \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mlaplacian_eigenvector_pe)\n\u001b[1;32m      2\u001b[0m explainer_pipeline\u001b[38;5;241m.\u001b[39mexplanations[\u001b[38;5;241m26\u001b[39m]\n",
      "File \u001b[0;32m~/code/paying-attention/src/models/explainer/explainer_pipeline.py:42\u001b[0m, in \u001b[0;36mExplainerPipeline.explain\u001b[0;34m(self, node_idx, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexplain\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanations[node_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39medge_index, index\u001b[38;5;241m=\u001b[39mnode_idx, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/explain/explainer.py:250\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m             explanation[key] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 250\u001b[0m explanation\u001b[38;5;241m.\u001b[39mvalidate_masks()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39mthreshold(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/explain/explanation.py:55\u001b[0m, in \u001b[0;36mExplanationMixin.validate_masks\u001b[0;34m(self, raise_on_error)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m store:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store\u001b[38;5;241m.\u001b[39medge_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     warn_or_raise(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with one dimension (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;241m.\u001b[39medge_mask\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, raise_on_error)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "explainer_pipeline.explain(26, laplacian_eigenvector_pe = data.laplacian_eigenvector_pe)\n",
    "explainer_pipeline.explanations[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_reduce(matrix):\n",
    "\n",
    "    #apply max reduction along columns (dim = 1)\n",
    "    max_values, _ = matrix.max(dim=1)\n",
    "    return max_values\n",
    "\n",
    "#finds the weighted_average for one singular attention matrix\n",
    "\n",
    "#when summing across dim=0, output shows how much attention each node receives, when dim=1, it shows how much attention each node is giving\n",
    "\n",
    "def weighted_average_received(attention_matrix):\n",
    "    \n",
    "    # Apply softmax across rows (dim=1) to normalize each row of the attention matrix\n",
    "    softmax_attention = F.softmax(attention_matrix, dim=1)\n",
    "    \n",
    "    # Compute the weighted average across each row (dim=1) by summing\n",
    "    weighted_avg = softmax_attention.sum(dim=0)  # Sum along the columns\n",
    "    \n",
    "    return weighted_avg\n",
    "\n",
    "def weighted_average_given(attention_matrix):\n",
    "\n",
    "    \"\"\"Computes the weighted average of an attention matrix to show how much attention each node is giving to others\"\"\"\n",
    "    \n",
    "    # Step 1: Apply softmax across rows (dim=1) to normalize attention\n",
    "    softmax_attention = F.softmax(attention_matrix, dim=1)\n",
    "    \n",
    "    # Step 2: Compute the weighted average across rows (dim=1)\n",
    "    # Multiply each value by its respective column index (weighted sum)\n",
    "    weighted_avg = torch.matmul(softmax_attention, torch.arange(attention_matrix.size(1), dtype=torch.float32))\n",
    "\n",
    "    return weighted_avg\n",
    "\n",
    "def weighted_average_all_layers(function, matrices):\n",
    "\n",
    "    #store all the weighted averages per matrix (from each layer)\n",
    "    weighted_averages = []\n",
    "\n",
    "    #use the weighted_average function (single matrix use case) in a loop to collect all the weighted averages,\n",
    "    #and append to list\n",
    "\n",
    "    for matrix in matrices:\n",
    "        current_weighted_avg = function(matrix)\n",
    "        weighted_averages.append(current_weighted_avg)\n",
    "\n",
    "    # Compute the average of the weighted averages across all layers\n",
    "    avg_all_matrices= torch.stack(weighted_averages).mean(dim=0)\n",
    "\n",
    "    return weighted_averages, avg_all_matrices\n",
    "\n",
    "def top_k_nodes(matrix, top_k=0.1):\n",
    "\n",
    "    # Rank nodes by importance (highest first)\n",
    "    sorted_indices = torch.argsort(matrix, descending=True)\n",
    "\n",
    "    # Select the top K nodes (either as a percentage or fixed number)\n",
    "    if isinstance(top_k, float):\n",
    "        top_k = int(len(sorted_indices) * top_k)  # Percentage to number of nodes\n",
    "    top_nodes = sorted_indices[:top_k]\n",
    "    return top_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
